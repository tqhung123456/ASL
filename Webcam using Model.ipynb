{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5cde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from spellchecker import SpellChecker\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35b9b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "from keras.models import load_model\n",
    "model = load_model('ASL landmarks using Dense v2.h5')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43baf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "mp_model = mp_hands.Hands(\n",
    "    static_image_mode=True,  # static images\n",
    "    max_num_hands=1,  # max 1 hands detection\n",
    "    min_detection_confidence=0.5)  # detection confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dcb9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_720p():\n",
    "    cap.set(3, 1280)\n",
    "    cap.set(4, 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1fe6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e2f9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results): \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe72e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_border(image, results):\n",
    "    h, w, c = image.shape\n",
    "    if results.left_hand_landmarks:\n",
    "        hand_landmarks = [results.left_hand_landmarks]\n",
    "    elif results.right_hand_landmarks:\n",
    "        hand_landmarks = [results.right_hand_landmarks]\n",
    "    else:\n",
    "        hand_landmarks = False\n",
    "        \n",
    "    x_max = 0\n",
    "    y_max = 0\n",
    "    x_min = w\n",
    "    y_min = h\n",
    "        \n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 105), 2) \n",
    "    return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9abb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    if results.left_hand_landmarks != None:\n",
    "        x = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    elif results.right_hand_landmarks != None:\n",
    "        x = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94af677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image(image, x_min, x_max, y_min, y_max):\n",
    "    height, width, color = image.shape\n",
    "    if(x_max - x_min >= y_max - y_min):\n",
    "        h = x_max - x_min\n",
    "        y_min = y_min - 25\n",
    "        y_max = y_min + h\n",
    "    else:\n",
    "        h = y_max - y_min\n",
    "        x_min = x_min - 25\n",
    "        x_max = x_min + h\n",
    "        \n",
    "    y_min = y_min - 25\n",
    "    y_max = y_max + 25\n",
    "    x_min = x_min - 25\n",
    "    x_max = x_max + 25\n",
    "    return image[y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f27c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_image(img):\n",
    "    img_size = 80\n",
    "    minValue = 70\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 2)\n",
    "    th3 = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    resized = np.int_(cv2.resize(res, (img_size, img_size)))\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e490a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keypoints(keypoints):\n",
    "    x = keypoints[0]*200\n",
    "    y = keypoints[1]*200\n",
    "    z = keypoints[2]*100\n",
    "    for j in range(63):\n",
    "        if j % 3 == 0:\n",
    "            keypoints[j] = keypoints[j]*200 - x\n",
    "        elif j % 3 == 1:\n",
    "            keypoints[j] = keypoints[j]*200 - y\n",
    "        else:\n",
    "            keypoints[j] = keypoints[j]*100 - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d0d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select language: 1 for Eng, 2 for Vietnamese\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "              'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "              'W', 'X', 'Y', 'Z', 'del', 'space']\n",
    "str = \"\"\n",
    "spell = SpellChecker()\n",
    "flag = False\n",
    "flag2 = False\n",
    "Lang = True\n",
    "Trans = False\n",
    "print('Please select language: 1 for Eng, 2 for Vietnamese')\n",
    "img_sequence = np.zeros((200,1200,3), np.uint8)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.8, min_tracking_confidence = 0.8) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Change resolution\n",
    "        # make_720p()\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        k = cv2.waitKey(1)\n",
    "        if k % 256 == 49:\n",
    "            print('Selected English')\n",
    "        elif k % 256 == 50:\n",
    "            print('Da chon tieng Viet')\n",
    "            Lang = False\n",
    "        if k % 256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "        # Make detections of hand\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.left_hand_landmarks == None and results.right_hand_landmarks == None:\n",
    "            index = 'Nothing'\n",
    "        else:\n",
    "            # Draw a box around hand\n",
    "            x_min, x_max, y_min, y_max = draw_border(image, results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # Cropping Image\n",
    "            image_crop = extract_image(frame, x_min, x_max, y_min, y_max)\n",
    "            \n",
    "            if k % 256 == 32:\n",
    "                img_sequence = np.zeros((200,1200,3), np.uint8)\n",
    "                # Extract keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                keypoints = keypoints.reshape(-1, 63)\n",
    "                \n",
    "                # Make prediction\n",
    "                prediction = np.argmax(model.predict(keypoints)[0])\n",
    "                index = letterpred[prediction]\n",
    "                if prediction != 26 and prediction != 27:\n",
    "                    str += index\n",
    "                else:\n",
    "                    if prediction == 27:\n",
    "                        correction = ' '.join([spell.correction(word) for word in str.split()]).upper()\n",
    "                        if correction != str and Lang:\n",
    "                            print('Do you mean:', correction)\n",
    "                            flag = True\n",
    "                        str += ' '\n",
    "                        flag2 = True\n",
    "                        print(\"Do you want to translate to Vietnamese.\")\n",
    "                    elif prediction == 26:\n",
    "                        str =  str[:-1]\n",
    "                cv2.putText(img_sequence, '%s' % (str), (30,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "        if k % 256 == 121 and flag:\n",
    "            str = correction\n",
    "            str += ' '\n",
    "            flag = False\n",
    "            img_sequence = np.zeros((200,1200,3), np.uint8)\n",
    "            cv2.putText(img_sequence, '%s' % (str), (30,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "        elif k % 256 == 110:\n",
    "            pass\n",
    "        if Lang and flag2 and k % 256 == 121:\n",
    "            res = ts.google(str, to_language='vi')\n",
    "            img_sequence = np.zeros((200,1200,3), np.uint8)\n",
    "            cv2.putText(img_sequence, '%s' % (res), (30,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "            flag2 = False\n",
    "        cv2.imshow('sequence', img_sequence)        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8757eee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'translators' has no attribute 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ts\u001b[39m.\u001b[39;49mgoogle(\u001b[39m\"\u001b[39m\u001b[39mI Love You\u001b[39m\u001b[39m\"\u001b[39m, to_language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvi\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'translators' has no attribute 'google'"
     ]
    }
   ],
   "source": [
    "ts.google(\"I Love You\", to_language='vi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
